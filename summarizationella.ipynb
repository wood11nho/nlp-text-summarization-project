{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Ac:\\Users\\stoic\\miniconda3\\envs\\myenv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3921: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Map: 100%|██████████| 52214/52214 [00:39<00:00, 1320.16 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Map: 100%|██████████| 13054/13054 [00:08<00:00, 1461.80 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "# Load the Romanian text summarization dataset\n",
    "data = load_dataset(\"readerbench/ro-text-summarization\")\n",
    "\n",
    "# Prepare and split the dataset\n",
    "train_val_split = data[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_val_split[\"train\"]\n",
    "validation_dataset = train_val_split[\"test\"]\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Function to tokenize the inputs and labels\n",
    "def preprocess_function(examples):\n",
    "    inputs = [doc for doc in examples[\"Content\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"Summary\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply the preprocessing function to training and validation data\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "validation_dataset = validation_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./results',          # Output directory for model checkpoints\n",
    "    evaluation_strategy='epoch',     # Evaluation is done at the end of each epoch\n",
    "    learning_rate=5e-5,              # Learning rate\n",
    "    per_device_train_batch_size=4,   # Training batch size\n",
    "    per_device_eval_batch_size=4,    # Evaluation batch size\n",
    "    num_train_epochs=3,              # Number of training epochs\n",
    "    weight_decay=0.01,               # Weight decay\n",
    "    save_total_limit=3,              # Only save the last three checkpoints\n",
    "    predict_with_generate=True       # Use model.generate for predictions\n",
    ")\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# Load the ROUGE metric to evaluate performance\n",
    "from datasets import load_metric\n",
    "rouge_metric = load_metric(\"rouge\")\n",
    "\n",
    "# Get predictions for validation dataset\n",
    "val_predictions = trainer.predict(validation_dataset)\n",
    "\n",
    "# Compute ROUGE scores\n",
    "rouge_scores = rouge_metric.compute(\n",
    "    predictions=[tokenizer.decode(output, skip_special_tokens=True) for output in val_predictions.predictions],\n",
    "    references=[item[\"summary\"] for item in validation_dataset]\n",
    ")\n",
    "\n",
    "print(\"ROUGE scores:\", rouge_scores)\n",
    "\n",
    "# Test the model with a sample input\n",
    "sample_text = \"\"\"Buzz House este un film care va fi lansat pe 23 aprilie 2024. Este o producție a studiourilor \"Star Films\", cunoscut pentru filmele sale cu teme inovatoare și regizori premiati. Filmul urmărește viața a patru personaje care locuiesc într-o casă zgomotoasă și plină de aventuri neprevăzute. Fiecare cameră a casei are o poveste diferită, cu decoruri bogate și detalii atent alese. Distributia include actori cunoscuți din industria de teatru și film, precum Ion Popescu, care a jucat în numeroase filme premiate la festivaluri internaționale.\n",
    "\n",
    "Maria Ionescu, de asemenea, face parte din distribuție și este cunoscută pentru rolurile sale în serialele de televiziune. Andrei Vasilescu, care a studiat la o școală de teatru celebră în New York, aduce o energie specială filmului. Elena Georgescu, ultimul nume important din distribuție, este actriță și cântăreață, cunoscută pentru concertele ei pline de viață.\n",
    "\n",
    "Regizorul filmului este recunoscut pentru stilul său unic, care îmbină comedia cu drama și elemente de film noir. În timpul filmărilor, a fost un entuziasm mare pe platou, cu toți actorii implicându-se în improvizații pentru a adăuga profunzime personajelor. Povestea filmului este plină de surprize, iar spectatorii se pot aștepta la râsete și lacrimi deopotrivă. În plus, coloana sonoră este creată de un compozitor premiat, ceea ce adaugă un element suplimentar de emoție.\n",
    "\n",
    "Casa în care s-a filmat este o proprietate veche, cu o istorie interesantă, și a fost aleasă pentru atmosfera sa unică. Regizorul a declarat că s-a inspirat din experiențe reale pentru a scrie scenariul. De asemenea, s-au folosit multe costume autentice din perioada filmului pentru a conferi autenticitate. Spectatorii pot aștepta scene memorabile și un final care îi va face să reflecteze asupra vieții lor. Filmul promite să fie unul dintre cele mai interesante ale anului.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize and generate a summary\n",
    "inputs = tokenizer(sample_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=100, min_length=20, do_sample=False)\n",
    "\n",
    "summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Sample text:\", sample_text)\n",
    "print(\"Generated summary:\", summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
